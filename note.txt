main 
-> spimi:start 
-> spimi:processFile	find doc and goto processDoc to handle the rest of the lines
-> spimi:processDoc	put terms of one document in set<string>
-> spimi:updateDict	for each term in set, insert into dict
-> dict:insert		
-> indexList:push




dict::writeToFile	for every <term, termId> in mp,
				write (	term_length, 
					term, 
					term_docFreq, 
					buf_size as offset,
					{all values/docIDs in buf}, 
					bit, 
					last docID
					)


merger::merge
-> merger::mergeTwo	read and compare term order and write and writeRest

spimi::generateDictIdx	idx write {all docIDs, bit, last docID}
->cd::generateDict	make dict (df, term_offset_in_dict, offset_in_idx)
->cd::writeToFile	dict write (term total length, all terms, {df, term_offset_in_dict, offset_in_idx})


--------change from gamma to fix length-----------


dict: dict of a block, it receives <term, docid>
	map<term, PostingsList>

postingsList:
	map<docID, term freq>

for every <term, PostingsList> in mp write {termLen, term, df, {docid, termfreq}}



postings list£º	for each term {docID, termfreq}, 4Byte+4Byte
dictionary:	for each term {offset in string, offset in posting list} 4Byte+4Byte
terms:		all terms as a string

dict sorted in lexicon order, do binary search (compare lexicon order) to find dict entry
may need to read 2 dictNodes to decide read size